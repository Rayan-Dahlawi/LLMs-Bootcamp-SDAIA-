{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rayan-Dahlawi/LLMs-Bootcamp-SDAIA-/blob/main/Rayan_Dahlawi_Assignment4_Understanding_the_Customer%E2%80%99s_Feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q24hSxA4tA6n"
      },
      "source": [
        "# Text classification: Understanding the Customer's Feedback\n",
        "\n",
        "---\n",
        "\n",
        "Text classification is one of the important tasks of text mining\n",
        "\n",
        "![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n",
        "\n",
        "In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review.\n",
        "\n",
        "We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O1jA8byt4bV"
      },
      "source": [
        "The libraries needed in this exercise are:\n",
        "* [Numpy](http://www.numpy.org/) — a package for scientific computing.\n",
        "* [Pandas](https://pandas.pydata.org/) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
        "* [Matplotlib](https://matplotlib.org/) — a package for plotting & visualizations.\n",
        "* [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
        "* [NLTK](http://www.nltk.org/) — a platform to work with natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "844CS6rf57X7"
      },
      "source": [
        "##Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAt6rj955meo"
      },
      "source": [
        "### Importing the libraries and necessary dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRN4WqkltlB5",
        "outputId": "18e84c6a-17fb-491a-80e6-09f463a95962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "# download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7duM74C95rhN"
      },
      "source": [
        "### Loading the dataset in our directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c48UYWDcg3hR",
        "outputId": "e84e8bb2-b644-4f81-f316-58d0f6ebda92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download IMDB dataset\n",
        "!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"\n",
        "\n",
        "# list files in current directory\n",
        "!ls -lah"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-14 10:43:19--  https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65862309 (63M) [text/plain]\n",
            "Saving to: ‘movie_data.csv’\n",
            "\n",
            "movie_data.csv      100%[===================>]  62.81M   191MB/s    in 0.3s    \n",
            "\n",
            "2023-11-14 10:43:21 (191 MB/s) - ‘movie_data.csv’ saved [65862309/65862309]\n",
            "\n",
            "total 63M\n",
            "drwxr-xr-x 1 root root 4.0K Nov 14 10:43 .\n",
            "drwxr-xr-x 1 root root 4.0K Nov 14 10:42 ..\n",
            "drwxr-xr-x 4 root root 4.0K Nov 10 14:21 .config\n",
            "-rw-r--r-- 1 root root  63M Nov 14 10:43 movie_data.csv\n",
            "drwxr-xr-x 1 root root 4.0K Nov 10 14:22 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77spW4xt5y4R"
      },
      "source": [
        "###Reading the dataset file and getting info on it\n",
        "**Question 1:** Use pandas to read the csv file and display the first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0A5QhDlteWj"
      },
      "source": [
        "# path to IMDB dataseet\n",
        "\n",
        "\n",
        "# read file (dataset) into our program using pandas\n",
        "\n",
        "\n",
        "# display first 5 rows\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8oHmgm-6qK2"
      },
      "source": [
        "Getting info on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQVx6AhqhAiB"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPbcG_8k54JZ"
      },
      "source": [
        "A balanced dataset in sentiment analysis is a dataset which holds an equal amount of positive sentiment data and negative sentiment data, meaning 50% of the data is positive and 50% is negative"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Check if dataset is balanced (number of positive sentiment = number of negative sentiment) by plotting the different classes"
      ],
      "metadata": {
        "id": "rgvEJ3BSK_7e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q12nMYY5vPhn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4uAuueIwKkS"
      },
      "source": [
        "## Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCxs0pSovUOa"
      },
      "source": [
        "print(data.review[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAvczEBgxUWl"
      },
      "source": [
        "**Question 3:** Let's define a function that would clean each movie review (sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKKIsHqZwRJR"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "english_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# define cleaning function\n",
        "def clean_review(text):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NIqPBfK67Zc"
      },
      "source": [
        "**Question 4 :** Try it out on an instance of the dataset then on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Bn3r1wzvwR"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Ycze9C6_yb"
      },
      "source": [
        "And now clean the entire dataset reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kHxWkPTz5eA"
      },
      "source": [
        "# apply to all dataset\n",
        "data['clean_review'] = data['review'].apply(clean_review)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkVqSSzu2Ax8"
      },
      "source": [
        "## Split dataset for training and testing\n",
        "We will split our data into two subsets: a 50% subset will be used for training the model for prediction and the remaining 50% will be used for evaluating or testing its performance. The random state ensures reproducibility of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Split your data to get x_train, x_test, y_train and y_test."
      ],
      "metadata": {
        "id": "HfMQ4DP0LahH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPHlwVS71brN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X =\n",
        "y =\n",
        "\n",
        "# Split data into 50% training & 50% test\n",
        "# Use a random state of 42 for example to ensure having the same split\n",
        "\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz23g0nD2nhN"
      },
      "source": [
        "## Feature extraction with Bag of Words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:**  In this section, apply the Bag of Words method to learn the vocabulary of your text and with it transform your training input data."
      ],
      "metadata": {
        "id": "FGHs66FILldh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_B0vrn-2sON"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# define a CountVectorizer (with binary=True and max_features=10000)\n",
        "\n",
        "\n",
        "# learn the vocabulary of all tokens in our training dataset\n",
        "\n",
        "\n",
        "# transform x_train to bag of words\n",
        "x_train_bow = vectorizer.transform(x_train)\n",
        "x_test_bow = vectorizer.transform(x_test)\n",
        "\n",
        "print(x_train_bow.shape, y_train.shape)\n",
        "print(x_test_bow.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtLaJfuw4060"
      },
      "source": [
        "## Classification\n",
        "\n",
        "**Question 7:** Your data is ready for classification. For this task use [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mS51YGO4hfv"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# define the LogisticRegression classifier\n",
        "\n",
        "\n",
        "# train the classifier on the training data\n",
        "\n",
        "\n",
        "# get the mean accuracy on the training data\n",
        "\n",
        "\n",
        "print('Training Accuracy:', acc_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Csw7GEm76E5"
      },
      "source": [
        "**Question 8:**  Evaluating the performance of your model through its accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJnyoqO5NyE"
      },
      "source": [
        "# Evaluate model with test data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh5927-d6Gq4"
      },
      "source": [
        "## Bonus: Let's use the model to predict!\n",
        "To do so, let's create a predict function which takes as argument your model and the bag of words vectorizer together with a review on which it would predict the sentiment. This review should be cleaned with the `clean_review` function we built, transformed by bag of words and then used for prediction with `model.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6kxkZ5m55Ii"
      },
      "source": [
        "# define predict function\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VrNunL18l4a"
      },
      "source": [
        "And let's try it out on an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6WCl916flD"
      },
      "source": [
        "review = 'The movie was great!'\n",
        "predict(model, vectorizer, review)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}